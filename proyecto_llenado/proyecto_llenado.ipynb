{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Proyecto 1 — Estación de llenado y taponado\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Importación de las librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Definir las rutas de los archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data\")\n",
    "\n",
    "telemetria_file = data_path / \"telemetria.csv\"\n",
    "eventos_file = data_path / \"eventos.csv\"\n",
    "botellas_file = data_path / \"botellas.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# FASE 1 --->Ingesta y validación (Pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "\n",
    "PASO 1: Carga y tipado de datos\n",
    "\n",
    "    1.1 Definir tipos de datos explícitos para cada CSV utilizando un diccionario "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtype_tel = {\n",
    "    'temp_prod': 'float32',\n",
    "    'vel_cinta': 'float32',\n",
    "    'caudal': 'float32',\n",
    "    'energia_kwh': 'float64'\n",
    "}\n",
    "\n",
    "dtype_evt = {\n",
    "    'tipo': 'category',\n",
    "    'id_botella': 'Int64'\n",
    "}\n",
    "\n",
    "dtype_pz = {\n",
    "    'id_botella': 'int64',\n",
    "    'peso_neto': 'float32',\n",
    "    'formato': 'category'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "    1.2 Cargar los csv en los dataFrames parseando el tiempo a datetime64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar telemetría con parseo de fecha\n",
    "df_tel = pd.read_csv(\n",
    "    telemetria_file,\n",
    "    dtype=dtype_tel,\n",
    "    parse_dates=['ts'],\n",
    "    date_format='ISO8601'\n",
    ")\n",
    "# Convertir ts a UTC y establecer como índice\n",
    "df_tel['ts'] = pd.to_datetime(df_tel['ts'], utc=True) # Convierte a datetime con zona horaria UTC\n",
    "df_tel = df_tel.set_index('ts').sort_index() # Establece la columna de tiempo como índice del DataFrame\n",
    "\n",
    "# Cargar eventos\n",
    "df_evt = pd.read_csv(\n",
    "    eventos_file,\n",
    "    dtype=dtype_evt,\n",
    "    parse_dates=['ts_ini', 'ts_fin'],\n",
    "    date_format='ISO8601'\n",
    ") # Lee el CSV y convierte automáticamente las columnas de fecha\n",
    "df_evt['ts_ini'] = pd.to_datetime(df_evt['ts_ini'], utc=True) #Convierte a datetime con zona horaria UTC\n",
    "df_evt['ts_fin'] = pd.to_datetime(df_evt['ts_fin'], utc=True) #Convierte a datetime con zona horaria UTC\n",
    "df_evt = df_evt.sort_values('ts_ini').reset_index(drop=True)\n",
    "\n",
    "# Cargar botellas\n",
    "df_pz = pd.read_csv(\n",
    "    botellas_file,\n",
    "    dtype=dtype_pz,\n",
    "    parse_dates=['ts_ciclo'],\n",
    "    date_format='ISO8601'\n",
    ")\n",
    "df_pz['ts_ciclo'] = pd.to_datetime(df_pz['ts_ciclo'], utc=True) #Convierte a datetime con zona horaria UTC\n",
    "df_pz = df_pz.sort_values('ts_ciclo').reset_index(drop=True) #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "PASO 2. Orden y duplicados\n",
    "\n",
    "    2.1 Telemetría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Telemetria:\")\n",
    "# Eliminar duplicados exactos en telemetría\n",
    "duplicados_antes_tel = df_tel.duplicated().sum()\n",
    "df_tel = df_tel[~df_tel.duplicated(keep='first')]\n",
    "\n",
    "#df_evt = df_evt[~df_evt.duplicated(keep='first')]\n",
    "\n",
    "# Verificar monotonía del índice\n",
    "es_monotono_tel = df_tel.index.is_monotonic_increasing #Comprobacion de la monotonia: Los indices temporales avanzan correctamente de menor a mayor\n",
    "\n",
    "print(f\"Duplicados eliminados: {duplicados_antes_tel}\")\n",
    "print(f\"Índice monótono: {es_monotono_tel}\")\n",
    "\n",
    "# Verificar si hay duplicados en el índice temporal\n",
    "duplicados_index = df_tel.index.duplicated().sum()\n",
    "if duplicados_index > 0:\n",
    "    print(f\"Hay {duplicados_index} timestamps duplicados en el índice\")\n",
    "    df_tel = df_tel[~df_tel.index.duplicated(keep='first')]\n",
    "else:\n",
    "    print(\"No hay timestamps duplicados\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "    2.2 Eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Eventos:\")\n",
    "# Eliminar duplicados exactos en telemetría\n",
    "dup_evt = df_evt.duplicated().sum()\n",
    "df_evt = df_evt[~df_evt.duplicated(keep='first')]\n",
    "\n",
    "# Ordenar por tiempo de inicio (y fin como desempate)\n",
    "df_evt = df_evt.sort_values(['ts_ini', 'ts_fin']).reset_index(drop=True)\n",
    "\n",
    "# Chequeos\n",
    "es_monotono_evt = df_evt['ts_ini'].is_monotonic_increasing\n",
    "neg_dur = (df_evt['ts_fin'] < df_evt['ts_ini']).sum()\n",
    "dup_ts_ini = df_evt['ts_ini'].duplicated().sum()\n",
    "\n",
    "print(f\"Duplicados eliminados: {dup_evt}\")\n",
    "print(f\"Orden por ts_ini monótono: {es_monotono_evt}\")\n",
    "print(f\"Eventos con ts_fin < ts_ini: {neg_dur}\")\n",
    "print(f\"Timestamps ts_ini duplicados: {dup_ts_ini}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "    2.3 Botellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Botellas:\")\n",
    "dup_pz = df_pz.duplicated().sum()\n",
    "df_pz = df_pz[~df_pz.duplicated(keep='first')]\n",
    "\n",
    "# Ordenar por ts_ciclo\n",
    "df_pz = df_pz.sort_values('ts_ciclo').reset_index(drop=True)\n",
    "\n",
    "# Chequeos\n",
    "es_monotono_pz = df_pz['ts_ciclo'].is_monotonic_increasing\n",
    "dup_ts_ciclo = df_pz['ts_ciclo'].duplicated().sum()\n",
    "dup_id_botella = df_pz['id_botella'].duplicated().sum()\n",
    "\n",
    "print(f\"Duplicados eliminados: {dup_pz}\")\n",
    "print(f\"Orden por ts_ciclo monótono: {es_monotono_pz}\")\n",
    "print(f\"Timestamps ts_ciclo duplicados: {dup_ts_ciclo}\")\n",
    "print(f\"id_botella duplicados: {dup_id_botella}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "PASO 3: Validaciones de rango\n",
    "\n",
    "    3.1 Marcar valores fuera de rango (sin eliminar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANGO_TEMP = (18.0,35.0)\n",
    "RANGO_VEL = (0.0,0.5)\n",
    "RANGO_CAUDAL = (0.0,12.0)\n",
    "\n",
    "df_tel['fuera_RANGO_TEMP'] = (df_tel['temp_prod'] < RANGO_TEMP[0]) | (df_tel['temp_prod'] > RANGO_TEMP[1])\n",
    "df_tel['fuera_RANGO_VEL'] = (df_tel['vel_cinta'] < RANGO_VEL[0]) | (df_tel['vel_cinta'] > RANGO_VEL[1])\n",
    "df_tel['fuera_RANGO_CAUDAL'] = (df_tel['caudal'] < RANGO_CAUDAL[0]) | (df_tel['caudal'] > RANGO_CAUDAL[1])\n",
    "\n",
    "n_temp_fuera = df_tel['fuera_RANGO_TEMP'].sum()\n",
    "n_vel_fuera = df_tel['fuera_RANGO_VEL'].sum()\n",
    "n_caudal_fuera = df_tel['fuera_RANGO_CAUDAL'].sum()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VALIDACIÓN DE RANGOS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"temp_prod fuera de [{RANGO_TEMP[0]}, {RANGO_TEMP[1]}] °C: {n_temp_fuera} ({n_temp_fuera/len(df_tel)*100:.2f}%)\")\n",
    "print(f\"vel_cinta fuera de [{RANGO_VEL[0]}, {RANGO_VEL[1]}] m/s: {n_vel_fuera} ({n_vel_fuera/len(df_tel)*100:.2f}%)\")\n",
    "print(f\"caudal fuera de [{RANGO_CAUDAL[0]}, {RANGO_CAUDAL[1]}] ml/s: {n_caudal_fuera} ({n_caudal_fuera/len(df_tel)*100:.2f}%)\")\n",
    "\n",
    "# Mostrar estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas:\")\n",
    "print(df_tel[['temp_prod', 'vel_cinta', 'caudal', 'energia_kwh']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "    3.2 Validar que energia_kwh no decrece (salvo cuantización)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular diferencias entre valores consecutivos de energía\n",
    "df_tel['delta_energia'] = df_tel['energia_kwh'].diff()\n",
    "\n",
    "# Contar cuántas veces la energía DECRECE (delta < 0)\n",
    "# Nota: diff() genera NaN en la primera fila, lo ignoramos\n",
    "decrementos = (df_tel['delta_energia'] < 0).sum() \n",
    "total_cambios = df_tel['delta_energia'].notna().sum() #Va a contar cuantas veces decrece\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VALIDACIÓN DE ENERGÍA NO DECRECIENTE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total de cambios: {total_cambios:,}\")\n",
    "print(f\"Decrementos detectados: {decrementos} ({decrementos/total_cambios*100:.3f}%)\")\n",
    "\n",
    "# Mostrar algunos ejemplos de decrementos (si existen)\n",
    "if decrementos > 0:\n",
    "    print(\"\\n Ejemplos de energía que decrece:\")\n",
    "    ejemplos_decremento = df_tel[df_tel['delta_energia'] < 0][['energia_kwh', 'delta_energia']].head(10)\n",
    "    print(ejemplos_decremento)\n",
    "    \n",
    "    # Estadísticas de los decrementos\n",
    "    print(\"\\n Estadísticas de los decrementos:\")\n",
    "    print(df_tel[df_tel['delta_energia'] < 0]['delta_energia'].describe())\n",
    "else:\n",
    "    print(\"\\n No se detectaron decrementos en energia_kwh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "PASO 4: Monotonicidad de energía (corrección de decrementos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 4: Corregir decrementos de energía (si los hubiera)\n",
    "# Guardamos la columna original para comparación\n",
    "df_tel['energia_kwh_original'] = df_tel['energia_kwh'].copy()\n",
    "\n",
    "# Identificar decrementos\n",
    "decrementos_mask = df_tel['delta_energia'] < 0\n",
    "n_correcciones = decrementos_mask.sum()\n",
    "\n",
    "if n_correcciones > 0:\n",
    "    print(f\"Se encontraron {n_correcciones} decrementos. Corrigiendo...\")\n",
    "    \n",
    "    # Hacer clip de deltas negativos a 0\n",
    "    df_tel['delta_energia_corregida'] = df_tel['delta_energia'].clip(lower=0)\n",
    "    \n",
    "    # Reconstruir energía acumulada desde el primer valor\n",
    "    energia_inicial = df_tel['energia_kwh'].iloc[0]\n",
    "    df_tel['energia_kwh'] = energia_inicial + df_tel['delta_energia_corregida'].fillna(0).cumsum()\n",
    "    \n",
    "    # Recalcular delta_energia con valores corregidos\n",
    "    df_tel['delta_energia'] = df_tel['energia_kwh'].diff()\n",
    "    \n",
    "    print(f\"{n_correcciones} correcciones aplicadas\")\n",
    "else:\n",
    "    print(\"No se requieren correcciones en energia_kwh\")\n",
    "    print(\"La señal es naturalmente monótona creciente\")\n",
    "\n",
    "# Verificación final\n",
    "decrementos_final = (df_tel['delta_energia'] < 0).sum()\n",
    "print(f\"\\nVerificación final: {decrementos_final} decrementos después de corrección\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "PASO 5: Frecuencia y huecos temporales\n",
    "\n",
    "    5.1 Confirmar frecuencia nominal de 1 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Analizar la frecuencia de muestreo\n",
    "print(\"=\"*60)\n",
    "print(\"ANÁLISIS DE FRECUENCIA DE MUESTREO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calcular diferencias de tiempo entre muestras consecutivas\n",
    "time_diffs = df_tel.index.to_series().diff()\n",
    "\n",
    "# Contar muestras con intervalo de 1 segundo\n",
    "intervalo_1s = time_diffs == pd.Timedelta(seconds=1)\n",
    "n_1s = intervalo_1s.sum()\n",
    "total = len(time_diffs) - 1  # -1 porque el primer valor es NaN\n",
    "\n",
    "print(f\"\\nMuestras con intervalo de 1s: {n_1s}/{total}\")\n",
    "\n",
    "# Identificar huecos (intervalos > 1s)\n",
    "huecos = time_diffs[time_diffs > pd.Timedelta(seconds=1)]\n",
    "n_huecos = len(huecos)\n",
    "\n",
    "print(f\"\\nHuecos detectados (intervalos > 1s): {n_huecos}\")\n",
    "\n",
    "if n_huecos > 0:\n",
    "    print(f\"\\nEstadísticas de los huecos:\")\n",
    "    print(huecos.describe())\n",
    "    \n",
    "    # Clasificar huecos\n",
    "    huecos_pequenos = huecos[huecos <= pd.Timedelta(seconds=10)]\n",
    "    huecos_grandes = huecos[huecos > pd.Timedelta(seconds=10)]\n",
    "    \n",
    "    print(f\"\\nHuecos pequeños (≤10s): {len(huecos_pequenos)}\")\n",
    "    print(f\"Huecos grandes (>10s): {len(huecos_grandes)}\")\n",
    "else:\n",
    "    print(f\"El numero de huecos es: {n_huecos}\")\n",
    "    print(\"La frecuencia nominal es de 1 Hz. Todos los saltos son de un segundo\")\n",
    "    print(\"No es necesario interpolar ni marcar segmentos invalidos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "    5.2 Reindexar a rejilla de 1 segundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Reindexar a rejilla regular de 1 segundo\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REINDEXACIÓN A REJILLA DE 1 SEGUNDO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Crear rejilla temporal de 1s desde el primer al último timestamp\n",
    "ts_inicio = df_tel.index.min()\n",
    "ts_fin = df_tel.index.max()\n",
    "rejilla_1s = pd.date_range(start=ts_inicio, end=ts_fin, freq='1s')\n",
    "\n",
    "print(f\"\\nRango temporal:\")\n",
    "print(f\"   Inicio: {ts_inicio}\")\n",
    "print(f\"   Fin: {ts_fin}\")\n",
    "print(f\"   Duración: {ts_fin - ts_inicio}\")\n",
    "\n",
    "print(f\"\\nTamaño de los datos:\")\n",
    "print(f\"   Muestras originales: {len(df_tel):,}\")\n",
    "print(f\"   Rejilla de 1s: {len(rejilla_1s):,}\")\n",
    "print(f\"   Diferencia (huecos): {len(rejilla_1s) - len(df_tel):,}\")\n",
    "\n",
    "# Reindexar el DataFrame a la rejilla de 1s\n",
    "df_tel = df_tel.reindex(rejilla_1s)\n",
    "\n",
    "print(f\"\\nDataFrame reindexado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "5.3 Rellenar huecos pequeños (≤10s) con interpolación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Rellenar huecos ≤ 10s\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RELLENADO DE HUECOS PEQUEÑOS (≤10s)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Identificar bloques de NaN consecutivos\n",
    "df_tel['es_nan'] = df_tel['temp_prod'].isna()\n",
    "df_tel['bloque_nan'] = (df_tel['es_nan'] != df_tel['es_nan'].shift()).cumsum()\n",
    "\n",
    "# Calcular tamaño de cada bloque de NaN\n",
    "tamano_bloques = df_tel[df_tel['es_nan']].groupby('bloque_nan').size()\n",
    "\n",
    "# Clasificar bloques\n",
    "bloques_pequenos = tamano_bloques[tamano_bloques <= 10]\n",
    "bloques_grandes = tamano_bloques[tamano_bloques > 10]\n",
    "\n",
    "print(f\"\\nBloques de NaN detectados:\")\n",
    "print(f\"   Total de bloques: {len(tamano_bloques)}\")\n",
    "print(f\"   Bloques ≤10s: {len(bloques_pequenos)} (se interpolarán)\")\n",
    "print(f\"   Bloques >10s: {len(bloques_grandes)} (se marcarán como inválidos)\")\n",
    "\n",
    "# Crear máscara para huecos pequeños (≤10s)\n",
    "mask_huecos_pequenos = df_tel['bloque_nan'].isin(bloques_pequenos.index) & df_tel['es_nan']\n",
    "\n",
    "# Interpolación lineal para temp_prod y caudal en huecos pequeños\n",
    "print(f\"\\nInterpolando temp_prod y caudal...\")\n",
    "df_tel.loc[mask_huecos_pequenos, 'temp_prod'] = df_tel['temp_prod'].interpolate(method='linear', limit=10)\n",
    "df_tel.loc[mask_huecos_pequenos, 'caudal'] = df_tel['caudal'].interpolate(method='linear', limit=10)\n",
    "\n",
    "# Forward-fill para vel_cinta (propagar último valor válido)\n",
    "print(f\"Forward-fill en vel_cinta...\")\n",
    "df_tel.loc[mask_huecos_pequenos, 'vel_cinta'] = df_tel['vel_cinta'].ffill(limit=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "    5.4 Marcar huecos grandes (>10s) como inválidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Marcar huecos grandes como inválidos\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MARCADO DE HUECOS GRANDES (>10s)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Crear columna para marcar segmentos inválidos\n",
    "mask_huecos_grandes = df_tel['bloque_nan'].isin(bloques_grandes.index) & df_tel['es_nan']\n",
    "df_tel['segmento_invalido'] = mask_huecos_grandes\n",
    "\n",
    "# Contar segundos marcados como inválidos\n",
    "n_invalidos = df_tel['segmento_invalido'].sum()\n",
    "total_segundos = len(df_tel)\n",
    "\n",
    "print(f\"\\nSegmentos marcados como inválidos:\")\n",
    "print(f\"   Total de segundos inválidos: {n_invalidos:,}\")\n",
    "print(f\"   Porcentaje: {n_invalidos/total_segundos*100:.2f}%\")\n",
    "\n",
    "if len(bloques_grandes) > 0:\n",
    "    print(f\"\\nDetalle de huecos grandes:\")\n",
    "    for i, (bloque_id, tamano) in enumerate(bloques_grandes.items(), 1):\n",
    "        inicio_hueco = df_tel[df_tel['bloque_nan'] == bloque_id].index.min()\n",
    "        fin_hueco = df_tel[df_tel['bloque_nan'] == bloque_id].index.max()\n",
    "        print(f\"   Hueco {i}: {tamano}s desde {inicio_hueco} hasta {fin_hueco}\")\n",
    "        if i >= 5:\n",
    "            print(f\"   ... y {len(bloques_grandes)-5} huecos más\")\n",
    "            break\n",
    "\n",
    "# Limpiar columnas auxiliares\n",
    "df_tel = df_tel.drop(columns=['es_nan', 'bloque_nan'])\n",
    "\n",
    "print(f\"\\n✅ Proceso de huecos completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "PASO 6: Detección de atípicos\n",
    "\n",
    "    6.1 Detección por z-score (umbral ±3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Detección de atípicos por z-score\n",
    "print(\"=\"*60)\n",
    "print(\"DETECCIÓN DE ATÍPICOS POR Z-SCORE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Umbral estándar: valores con |z-score| > 3 son atípicos\n",
    "UMBRAL_Z = 3\n",
    "\n",
    "# Calcular z-score para cada variable\n",
    "# z-score = (valor - media) / desviación estándar\n",
    "df_tel['z_temp'] = (df_tel['temp_prod'] - df_tel['temp_prod'].mean()) / df_tel['temp_prod'].std()\n",
    "df_tel['z_vel'] = (df_tel['vel_cinta'] - df_tel['vel_cinta'].mean()) / df_tel['vel_cinta'].std()\n",
    "df_tel['z_caudal'] = (df_tel['caudal'] - df_tel['caudal'].mean()) / df_tel['caudal'].std()\n",
    "\n",
    "# Marcar atípicos (|z| > 3)\n",
    "df_tel['atipico_z_temp'] = df_tel['z_temp'].abs() > UMBRAL_Z\n",
    "df_tel['atipico_z_vel'] = df_tel['z_vel'].abs() > UMBRAL_Z\n",
    "df_tel['atipico_z_caudal'] = df_tel['z_caudal'].abs() > UMBRAL_Z\n",
    "\n",
    "# Contar atípicos detectados\n",
    "n_atip_temp = df_tel['atipico_z_temp'].sum()\n",
    "n_atip_vel = df_tel['atipico_z_vel'].sum()\n",
    "n_atip_caudal = df_tel['atipico_z_caudal'].sum()\n",
    "\n",
    "print(f\"\\nAtípicos detectados (|z-score| > {UMBRAL_Z}):\")\n",
    "print(f\"   temp_prod: {n_atip_temp} ({n_atip_temp/len(df_tel)*100:.3f}%)\")\n",
    "print(f\"   vel_cinta: {n_atip_vel} ({n_atip_vel/len(df_tel)*100:.3f}%)\")\n",
    "print(f\"   caudal: {n_atip_caudal} ({n_atip_caudal/len(df_tel)*100:.3f}%)\")\n",
    "\n",
    "# Mostrar ejemplos si existen\n",
    "if n_atip_temp > 0:\n",
    "    print(\"\\nEjemplos de atípicos en temp_prod:\")\n",
    "    print(df_tel[df_tel['atipico_z_temp']][['temp_prod', 'z_temp']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "    6.2 Detección por IQR (rango intercuartílico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Detección de atípicos por IQR\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETECCIÓN DE ATÍPICOS POR IQR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calcular cuartiles y rango intercuartílico (IQR)\n",
    "# IQR = Q3 - Q1\n",
    "# Límites: [Q1 - 1.5*IQR, Q3 + 1.5*IQR]\n",
    "\n",
    "for var in ['temp_prod', 'vel_cinta', 'caudal']:\n",
    "    Q1 = df_tel[var].quantile(0.25)\n",
    "    Q3 = df_tel[var].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    limite_inferior = Q1 - 1.5 * IQR\n",
    "    limite_superior = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Marcar atípicos\n",
    "    col_name = f'atipico_iqr_{var.split(\"_\")[0]}'  # atipico_iqr_temp, atipico_iqr_vel, atipico_iqr_caudal\n",
    "    df_tel[col_name] = (df_tel[var] < limite_inferior) | (df_tel[var] > limite_superior)\n",
    "    \n",
    "    n_atipicos = df_tel[col_name].sum()\n",
    "    \n",
    "    print(f\"\\n{var}:\")\n",
    "    print(f\"   Q1: {Q1:.3f}\")\n",
    "    print(f\"   Q3: {Q3:.3f}\")\n",
    "    print(f\"   IQR: {IQR:.3f}\")\n",
    "    print(f\"   Límites: [{limite_inferior:.3f}, {limite_superior:.3f}]\")\n",
    "    print(f\"   Atípicos: {n_atipicos} ({n_atipicos/len(df_tel)*100:.3f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "    6.3 Consolidar marcas de atípicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Consolidar detección de atípicos\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONSOLIDACIÓN DE ATÍPICOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Crear columna que marca si hay algún atípico (OR lógico)\n",
    "# Un registro es atípico si al menos una variable lo es (por cualquier método)\n",
    "df_tel['es_atipico'] = (\n",
    "    df_tel['atipico_z_temp'] | df_tel['atipico_z_vel'] | df_tel['atipico_z_caudal'] |\n",
    "    df_tel['atipico_iqr_temp'] | df_tel['atipico_iqr_vel'] | df_tel['atipico_iqr_caudal']\n",
    ")\n",
    "\n",
    "total_atipicos = df_tel['es_atipico'].sum()\n",
    "porcentaje = total_atipicos / len(df_tel) * 100\n",
    "\n",
    "print(f\"\\nRegistros con al menos un valor atípico:\")\n",
    "print(f\"   Total: {total_atipicos:,}\")\n",
    "print(f\"   Porcentaje: {porcentaje:.2f}%\")\n",
    "\n",
    "# Resumen por método\n",
    "print(f\"\\nComparación de métodos:\")\n",
    "print(f\"   Z-score: {(df_tel['atipico_z_temp'] | df_tel['atipico_z_vel'] | df_tel['atipico_z_caudal']).sum():,}\")\n",
    "print(f\"   IQR: {(df_tel['atipico_iqr_temp'] | df_tel['atipico_iqr_vel'] | df_tel['atipico_iqr_caudal']).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "PASO 7: Etiqueta RUN/STOP por segundo\n",
    "\n",
    "    7.1 Construir máscara de paradas desde eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Construir máscara STOP_evt desde eventos.csv\n",
    "print(\"=\"*60)\n",
    "print(\"CONSTRUCCIÓN DE MÁSCARA RUN/STOP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filtrar eventos que implican parada\n",
    "eventos_parada = df_evt[df_evt['tipo'].isin(['micro_parada', 'cambio_formato', 'limpieza'])].copy()\n",
    "\n",
    "print(f\"\\nEventos de parada encontrados:\")\n",
    "print(f\"   Total: {len(eventos_parada)}\")\n",
    "print(f\"   micro_parada: {(eventos_parada['tipo'] == 'micro_parada').sum()}\")\n",
    "print(f\"   cambio_formato: {(eventos_parada['tipo'] == 'cambio_formato').sum()}\")\n",
    "print(f\"   limpieza: {(eventos_parada['tipo'] == 'limpieza').sum()}\")\n",
    "\n",
    "# Inicializar columna STOP_evt en False (por defecto está en marcha)\n",
    "df_tel['STOP_evt'] = False\n",
    "\n",
    "# Marcar como True los segundos que caen en intervalos [ts_ini, ts_fin)\n",
    "for idx, evento in eventos_parada.iterrows():\n",
    "    mascara_tiempo = (df_tel.index >= evento['ts_ini']) & (df_tel.index < evento['ts_fin'])\n",
    "    df_tel.loc[mascara_tiempo, 'STOP_evt'] = True\n",
    "\n",
    "n_stop_evt = df_tel['STOP_evt'].sum()\n",
    "print(f\"\\nSegundos marcados como STOP por eventos: {n_stop_evt:,} ({n_stop_evt/len(df_tel)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "7.2 Definir RUN basado en velocidad de cinta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Definir RUN_vel basado en velocidad de cinta\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEFINICIÓN DE RUN_vel\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Umbral de velocidad para considerar que la máquina está en marcha\n",
    "UMBRAL_VEL_RUN = 0.05  # m/s\n",
    "\n",
    "# RUN_vel = True si vel_cinta >= 0.05 m/s\n",
    "df_tel['RUN_vel'] = df_tel['vel_cinta'] >= UMBRAL_VEL_RUN\n",
    "\n",
    "n_run_vel = df_tel['RUN_vel'].sum()\n",
    "print(f\"\\nUmbral de velocidad: {UMBRAL_VEL_RUN} m/s\")\n",
    "print(f\"Segundos con RUN_vel=True: {n_run_vel:,}\")\n",
    "print(f\"Segundos con RUN_vel=False: {len(df_tel)-n_run_vel:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "7.3 Combinar en estado final (RUN/STOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Definir estado final: RUN si RUN_vel=True Y STOP_evt=False\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMBINACIÓN DE CONDICIONES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# estado = RUN si (RUN_vel AND NOT STOP_evt), STOP en otro caso\n",
    "df_tel['estado'] = 'STOP'\n",
    "df_tel.loc[df_tel['RUN_vel'] & ~df_tel['STOP_evt'], 'estado'] = 'RUN'\n",
    "\n",
    "# Convertir a tipo category para ahorrar memoria\n",
    "df_tel['estado'] = df_tel['estado'].astype('category')\n",
    "\n",
    "# Contar estados\n",
    "n_run = (df_tel['estado'] == 'RUN').sum()\n",
    "n_stop = (df_tel['estado'] == 'STOP').sum()\n",
    "\n",
    "print(f\"\\nDistribución de estados:\")\n",
    "print(f\"   RUN: {n_run:,}\")\n",
    "print(f\"   STOP: {n_stop:,}\")\n",
    "\n",
    "# Análisis de transiciones\n",
    "df_tel['cambio_estado'] = df_tel['estado'] != df_tel['estado'].shift()\n",
    "n_transiciones = df_tel['cambio_estado'].sum() - 1  # -1 para excluir el primer valor\n",
    "\n",
    "print(f\"\\nTransiciones de estado detectadas: {n_transiciones}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "PASO 8: Agregación a 1 minuto (diagnóstico temprano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 8: Agregación temporal a 1 minuto\n",
    "print(\"=\"*60)\n",
    "print(\"AGREGACIÓN A 1 MINUTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Crear agregaciones por minuto\n",
    "df_1min = df_tel.resample('1min').agg({\n",
    "    'temp_prod': ['mean', lambda x: x.quantile(0.95)],\n",
    "    'caudal': 'mean',\n",
    "    'vel_cinta': 'mean',\n",
    "    'energia_kwh': 'last',  # Último valor del minuto (acumulado)\n",
    "    'estado': lambda x: (x == 'STOP').sum()  # Contar segundos en STOP\n",
    "}).round(3)\n",
    "\n",
    "# Aplanar nombres de columnas\n",
    "df_1min.columns = ['temp_mean', 'temp_p95', 'caudal_mean', 'vel_cinta_mean', 'energia_kwh', 'segundos_stop']\n",
    "\n",
    "# Calcular métricas derivadas\n",
    "df_1min['pct_stop'] = (df_1min['segundos_stop'] / 60 * 100).round(2)\n",
    "df_1min['segundos_run'] = 60 - df_1min['segundos_stop']\n",
    "df_1min['pct_run'] = (df_1min['segundos_run'] / 60 * 100).round(2)\n",
    "\n",
    "# Calcular delta de energía por minuto\n",
    "df_1min['delta_energia_min'] = df_1min['energia_kwh'].diff()\n",
    "\n",
    "# Información del resultado\n",
    "print(f\"\\nDataFrame agregado:\")\n",
    "print(f\"   Registros originales (1s): {len(df_tel):,}\")\n",
    "print(f\"   Registros agregados (1min): {len(df_1min):,}\")\n",
    "print(f\"   Rango temporal: {df_1min.index.min()} a {df_1min.index.max()}\")\n",
    "\n",
    "print(f\"\\nColumnas creadas:\")\n",
    "for col in df_1min.columns:\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "print(f\"\\nEstadísticas de disponibilidad:\")\n",
    "print(f\"   Media % RUN por minuto: {df_1min['pct_run'].mean():.2f}%\")\n",
    "print(f\"   Media % STOP por minuto: {df_1min['pct_stop'].mean():.2f}%\")\n",
    "print(f\"   Minutos con 100% RUN: {(df_1min['pct_run'] == 100).sum()} ({(df_1min['pct_run'] == 100).sum()/len(df_1min)*100:.2f}%)\")\n",
    "print(f\"   Minutos con 100% STOP: {(df_1min['pct_stop'] == 100).sum()} ({(df_1min['pct_stop'] == 100).sum()/len(df_1min)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nPrimeros registros:\")\n",
    "print(df_1min.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "# FASE 2: Ingeniería de variables y KPIs (NumPy + Pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "PASO 1: Cálculo de potencia instantánea desde energía acumulada\n",
    "\n",
    "    Fórmula: P_kW = ΔE / Δt (donde Δt está en horas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Fórmulas implementadas\n",
    "\n",
    "**Cálculo de potencia instantánea:**\n",
    "\n",
    "$$\\Delta E_i = \\max\\{E_i - E_{i-1}, 0\\}$$\n",
    "\n",
    "$$\\Delta t_i = \\frac{t_i - t_{i-1}}{3600} \\text{ (horas)}$$\n",
    "\n",
    "$$P_{\\text{kW},i} = \\frac{\\Delta E_i}{\\Delta t_i}$$\n",
    "\n",
    "$$P_{\\text{W},i} = 1000 \\cdot P_{\\text{kW},i}$$\n",
    "\n",
    "**Suavizado opcional (media móvil):**\n",
    "- Ventana: 5 segundos (centrada)\n",
    "- Objetivo: Mitigar efectos de cuantización del contador de energía"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 9: Calcular potencia instantánea\n",
    "print(\"=\"*60)\n",
    "print(\"CÁLCULO DE POTENCIA INSTANTÁNEA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calcular delta de energía (ya lo teníamos del Paso 4)\n",
    "# df_tel['delta_energia'] ya existe\n",
    "\n",
    "# Calcular delta de tiempo en horas\n",
    "df_tel['delta_tiempo_h'] = df_tel.index.to_series().diff().dt.total_seconds() / 3600\n",
    "\n",
    "# Calcular potencia en kW: P = ΔE / Δt\n",
    "# Evitar división por cero\n",
    "df_tel['P_kW'] = np.where(\n",
    "    df_tel['delta_tiempo_h'] > 0,\n",
    "    df_tel['delta_energia'] / df_tel['delta_tiempo_h'],\n",
    "    0\n",
    ")\n",
    "\n",
    "# Suavizar potencia con media móvil de 5 segundos para mitigar cuantización\n",
    "df_tel['P_kW_suavizada'] = df_tel['P_kW'].rolling(window=5, center=True, min_periods=1).mean()\n",
    "\n",
    "# Mostrar resultados de las fórmulas aplicadas\n",
    "print(\"\\nPrimeros 10 valores calculados:\")\n",
    "print(df_tel[['energia_kwh', 'delta_energia', 'delta_tiempo_h', 'P_kW', 'P_kW_suavizada']].head(10))\n",
    "\n",
    "print(f\"\\n✅ Potencia instantánea calculada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "PASO 2: Agregación a 1 minuto (telemetría)\n",
    "\n",
    "### Fórmulas de agregación\n",
    "\n",
    "Para cada minuto $m$:\n",
    "\n",
    "$$\\text{temp\\_mean}(m) = \\text{mean}(T)$$\n",
    "\n",
    "$$\\text{temp\\_p95}(m) = \\text{p95}(T)$$\n",
    "\n",
    "$$\\text{caudal\\_mean}(m) = \\text{mean}(q)$$\n",
    "\n",
    "$$\\text{P\\_kW\\_mean}(m) = \\text{mean}(P)$$\n",
    "\n",
    "$$\\%\\text{STOP}(m) = 100 \\cdot \\frac{\\#\\{i \\in m : \\text{estado}_i = \\text{STOP}\\}}{60}$$\n",
    "\n",
    "Estas series minuto servirán como base para KPIs horarios/por turno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 10: Agregación a 1 minuto (telemetría)\n",
    "print(\"=\"*60)\n",
    "print(\"AGREGACIÓN A 1 MINUTO (TELEMETRÍA)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Crear agregaciones por minuto aplicando las fórmulas\n",
    "df_1min = df_tel.resample('1min').agg({\n",
    "    'temp_prod': ['mean', lambda x: x.quantile(0.95)],\n",
    "    'caudal': 'mean',\n",
    "    'P_kW': 'mean',\n",
    "    'estado': lambda x: (x == 'STOP').sum()\n",
    "}).round(3)\n",
    "\n",
    "# Aplanar nombres de columnas\n",
    "df_1min.columns = ['temp_mean', 'temp_p95', 'caudal_mean', 'P_kW_mean', 'segundos_stop']\n",
    "\n",
    "# Calcular %STOP\n",
    "df_1min['pct_STOP'] = ((df_1min['segundos_stop'] / 60) * 100).round(2)\n",
    "\n",
    "print(f\"\\nPrimeros registros:\")\n",
    "print(df_1min.head(10))\n",
    "\n",
    "print(f\"\\n✅ Agregación a 1 minuto completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "PASO 3: Clasificación de botellas por tolerancia de peso\n",
    "\n",
    "### Objetivo de masa por formato\n",
    "\n",
    "$$m_{\\text{obj}}(250) = 250\\text{ g}, \\quad m_{\\text{obj}}(500) = 500\\text{ g}$$\n",
    "\n",
    "### Criterio de tolerancia\n",
    "\n",
    "Con tolerancia típica del ±2%, una unidad está dentro de tolerancia si:\n",
    "\n",
    "$$|\\text{peso\\_lleno\\_g} - m_{\\text{obj}}(f)| \\leq 0.02 \\cdot m_{\\text{obj}}(f)$$\n",
    "\n",
    "Donde:\n",
    "- $\\text{peso\\_lleno\\_g}$ es el peso neto de la botella (en gramos)\n",
    "- $f$ es el formato (250 o 500)\n",
    "- $m_{\\text{obj}}(f)$ es la masa objetivo según el formato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 11: Clasificación de botellas por tolerancia de peso\n",
    "print(\"=\"*60)\n",
    "print(\"CLASIFICACIÓN DE BOTELLAS POR TOLERANCIA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Definir masa objetivo según formato\n",
    "MASA_OBJ = {250: 250.0, 500: 500.0}\n",
    "TOLERANCIA = 0.02  # ±2%\n",
    "\n",
    "# Crear columna con masa objetivo según el formato de cada botella\n",
    "df_pz['masa_objetivo'] = df_pz['formato_ml'].map(MASA_OBJ)\n",
    "\n",
    "# Calcular desviación absoluta respecto al objetivo\n",
    "df_pz['desviacion_abs'] = np.abs(df_pz['peso_lleno_g'] - df_pz['masa_objetivo'])\n",
    "\n",
    "# Calcular límite de tolerancia (2% de la masa objetivo)\n",
    "df_pz['limite_tolerancia'] = TOLERANCIA * df_pz['masa_objetivo']\n",
    "\n",
    "# Clasificar: dentro_tolerancia = True si |peso_neto - m_obj| ≤ 0.02 * m_obj\n",
    "df_pz['dentro_tolerancia'] = df_pz['desviacion_abs'] <= df_pz['limite_tolerancia']\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"\\nPrimeros registros clasificados:\")\n",
    "print(df_pz[['id_botella', 'formato_ml', 'peso_lleno_g', 'masa_objetivo', 'desviacion_abs', 'dentro_tolerancia']].head(10))\n",
    "\n",
    "print(f\"\\n✅ Clasificación completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 11: Clasificación de botellas por tolerancia de peso\n",
    "print(\"=\"*60)\n",
    "print(\"CLASIFICACIÓN DE BOTELLAS POR TOLERANCIA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================================\n",
    "# PARTE A: KPIs POR HORA\n",
    "# ============================================================================\n",
    "print(\"\\n--- KPIs POR HORA ---\")\n",
    "\n",
    "# 1. Preparar df_pz con índice temporal\n",
    "df_pz_idx = df_pz.set_index('ts_ciclo')\n",
    "\n",
    "# 2. Agregar botellas por hora\n",
    "kpis_hora_pz = df_pz_idx.resample('1h').agg({\n",
    "    'id_botella': 'count',  # N_W: Total de botellas\n",
    "    'dentro_tolerancia': ['sum', lambda x: (~x).sum()]  # OK y NG\n",
    "})\n",
    "\n",
    "# Aplanar nombres de columnas\n",
    "kpis_hora_pz.columns = ['N_botellas', 'OK', 'NG']\n",
    "\n",
    "# 3. Agregar telemetría por hora\n",
    "kpis_hora_tel = df_tel.resample('1h').agg({\n",
    "    'estado': lambda x: (x == 'RUN').sum() / 3600,  # Tiempo en RUN (horas)\n",
    "    'energia_kwh': ['first', 'last']  # Energía inicial y final\n",
    "})\n",
    "\n",
    "kpis_hora_tel.columns = ['horas_RUN', 'energia_ini', 'energia_fin']\n",
    "kpis_hora_tel['delta_energia_kWh'] = kpis_hora_tel['energia_fin'] - kpis_hora_tel['energia_ini']\n",
    "\n",
    "# 4. Combinar ambos DataFrames\n",
    "kpis_hora = pd.concat([kpis_hora_pz, kpis_hora_tel], axis=1)\n",
    "\n",
    "# 5. Calcular KPIs\n",
    "kpis_hora['throughput_ud_h'] = np.where(\n",
    "    kpis_hora['N_botellas'] > 0,\n",
    "    kpis_hora['N_botellas'] / 1.0,  # Dividir por 1 hora\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "kpis_hora['scrap_pct'] = np.where(\n",
    "    kpis_hora['N_botellas'] > 0,\n",
    "    100 * kpis_hora['NG'] / kpis_hora['N_botellas'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "kpis_hora['energia_Wh_ud'] = np.where(\n",
    "    kpis_hora['N_botellas'] > 0,\n",
    "    1000 * kpis_hora['delta_energia_kWh'] / kpis_hora['N_botellas'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "kpis_hora['pct_tolerancia'] = np.where(\n",
    "    kpis_hora['N_botellas'] > 0,\n",
    "    100 * kpis_hora['OK'] / kpis_hora['N_botellas'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Redondear\n",
    "kpis_hora = kpis_hora.round(2)\n",
    "\n",
    "print(f\"\\nKPIs por hora calculados: {len(kpis_hora)} horas\")\n",
    "print(f\"\\nPrimeras horas:\")\n",
    "print(kpis_hora[['N_botellas', 'throughput_ud_h', 'scrap_pct', 'horas_RUN', 'energia_Wh_ud', 'pct_tolerancia']].head(10))\n",
    "\n",
    "# ============================================================================\n",
    "# PARTE B: KPIs POR TURNO\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"--- KPIs POR TURNO ---\")\n",
    "\n",
    "# Definir turnos (ejemplo: 06:00-14:00, 14:00-22:00, 22:00-06:00)\n",
    "def asignar_turno(hora):\n",
    "    if 6 <= hora < 14:\n",
    "        return 'T1_Mañana'\n",
    "    elif 14 <= hora < 22:\n",
    "        return 'T2_Tarde'\n",
    "    else:\n",
    "        return 'T3_Noche'\n",
    "\n",
    "# Asignar turno a cada botella\n",
    "df_pz['turno'] = df_pz['ts_ciclo'].dt.hour.apply(asignar_turno)\n",
    "df_pz['fecha'] = df_pz['ts_ciclo'].dt.date\n",
    "\n",
    "# Asignar turno a telemetría\n",
    "df_tel['turno'] = df_tel.index.hour.map(asignar_turno)\n",
    "df_tel['fecha'] = df_tel.index.date\n",
    "\n",
    "# Agregar por fecha y turno (botellas)\n",
    "kpis_turno_pz = df_pz.groupby(['fecha', 'turno']).agg({\n",
    "    'id_botella': 'count',\n",
    "    'dentro_tolerancia': ['sum', lambda x: (~x).sum()]\n",
    "})\n",
    "\n",
    "kpis_turno_pz.columns = ['N_botellas', 'OK', 'NG']\n",
    "\n",
    "# Agregar por fecha y turno (telemetría)\n",
    "kpis_turno_tel = df_tel.groupby(['fecha', 'turno']).agg({\n",
    "    'estado': lambda x: (x == 'RUN').sum() / 3600,\n",
    "    'energia_kwh': ['first', 'last']\n",
    "})\n",
    "\n",
    "kpis_turno_tel.columns = ['horas_RUN', 'energia_ini', 'energia_fin']\n",
    "kpis_turno_tel['delta_energia_kWh'] = kpis_turno_tel['energia_fin'] - kpis_turno_tel['energia_ini']\n",
    "\n",
    "# Combinar\n",
    "kpis_turno = pd.concat([kpis_turno_pz, kpis_turno_tel], axis=1)\n",
    "\n",
    "# Calcular KPIs por turno (8 horas por turno)\n",
    "kpis_turno['throughput_ud_h'] = np.where(\n",
    "    kpis_turno['N_botellas'] > 0,\n",
    "    kpis_turno['N_botellas'] / 8.0,\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "kpis_turno['scrap_pct'] = np.where(\n",
    "    kpis_turno['N_botellas'] > 0,\n",
    "    100 * kpis_turno['NG'] / kpis_turno['N_botellas'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "kpis_turno['energia_Wh_ud'] = np.where(\n",
    "    kpis_turno['N_botellas'] > 0,\n",
    "    1000 * kpis_turno['delta_energia_kWh'] / kpis_turno['N_botellas'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "kpis_turno['pct_tolerancia'] = np.where(\n",
    "    kpis_turno['N_botellas'] > 0,\n",
    "    100 * kpis_turno['OK'] / kpis_turno['N_botellas'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Redondear\n",
    "kpis_turno = kpis_turno.round(2)\n",
    "\n",
    "print(f\"\\nKPIs por turno calculados: {len(kpis_turno)} turnos\")\n",
    "print(f\"\\nPrimeros turnos:\")\n",
    "print(kpis_turno[['N_botellas', 'throughput_ud_h', 'scrap_pct', 'horas_RUN', 'energia_Wh_ud', 'pct_tolerancia']].head(10))\n",
    "\n",
    "print(f\"\\n✅ KPIs por hora y turno calculados correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "PASO 5: Cálculo del OEE - Opción A (Por tiempos y ciclo nominal)\n",
    "\n",
    "### Fórmula del OEE\n",
    "\n",
    "$$\\text{OEE}(W) = \\text{Availability}(W) \\times \\text{Performance}(W) \\times \\text{Quality}(W)$$\n",
    "\n",
    "---\n",
    "\n",
    "### Componentes del OEE\n",
    "\n",
    "#### 1. Availability (Disponibilidad)\n",
    "\n",
    "$$\\text{Availability}(W) = \\frac{t_{\\text{RUN}}(W)}{t_{\\text{plan}}(W)}$$\n",
    "\n",
    "**Interpretación:** Proporción del tiempo planificado que la máquina estuvo en marcha.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Performance (Rendimiento)\n",
    "\n",
    "$$\\text{Performance}(W) \\approx \\frac{t_{\\text{nom}}(W)}{t_{\\text{medio\\_RUN}}(W)}$$\n",
    "\n",
    "Donde:\n",
    "- $t_{\\text{nom}}(W)$ = tiempo de ciclo nominal ponderado por formato en $W$\n",
    "- $t_{\\text{medio\\_RUN}}(W)$ = tiempo medio de ciclo durante RUN en $W$\n",
    "\n",
    "**Interpretación:** Qué tan rápido producimos vs. la velocidad teórica.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Quality (Calidad)\n",
    "\n",
    "$$\\text{Quality}(W) = \\frac{OK_W}{OK_W + NG_W}$$\n",
    "\n",
    "**Interpretación:** Proporción de piezas buenas sobre el total producido.\n",
    "\n",
    "---\n",
    "\n",
    "### Parámetros\n",
    "\n",
    "- $t_{\\text{nom}} = 1.5$ s/botella (equivale a 2400 botellas/hora)\n",
    "- $t_{\\text{plan}} = 1$ hora (para ventanas horarias) ó $8$ horas (para turnos)\n",
    "\n",
    "---\n",
    "\n",
    "### Consideraciones de implementación\n",
    "\n",
    "- Si $N_W = 0$ o $t_{\\text{RUN}}(W) = 0$ → devolver `NaN`\n",
    "- Availability ya calculada en PASO 4 como `horas_RUN / horas_planificadas`\n",
    "- Quality ya calculada en PASO 4 como `pct_tolerancia / 100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 12: Cálculo del OEE - Opción A\n",
    "print(\"=\"*60)\n",
    "print(\"CÁLCULO DEL OEE - OPCIÓN A\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================================\n",
    "# PARÁMETROS\n",
    "# ============================================================================\n",
    "T_NOM = 1.5  # segundos/botella\n",
    "HORAS_PLANIFICADAS_HORA = 1.0  # 1 hora\n",
    "HORAS_PLANIFICADAS_TURNO = 8.0  # 8 horas por turno\n",
    "\n",
    "print(f\"\\nParámetros:\")\n",
    "print(f\"   t_nom: {T_NOM} s/botella\")\n",
    "print(f\"   Horas planificadas (hora): {HORAS_PLANIFICADAS_HORA} h\")\n",
    "print(f\"   Horas planificadas (turno): {HORAS_PLANIFICADAS_TURNO} h\")\n",
    "\n",
    "# ============================================================================\n",
    "# PARTE A: OEE POR HORA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"--- OEE POR HORA ---\")\n",
    "\n",
    "# 1. Availability = horas_RUN / horas_planificadas\n",
    "kpis_hora['Availability'] = kpis_hora['horas_RUN'] / HORAS_PLANIFICADAS_HORA\n",
    "\n",
    "# 2. Performance = t_nom / t_medio_RUN\n",
    "#    t_medio_RUN = horas_RUN / N_botellas (en horas/botella)\n",
    "#    Convertir t_nom a horas: 1.5 s = 1.5/3600 horas\n",
    "kpis_hora['Performance'] = np.where(\n",
    "    (kpis_hora['N_botellas'] > 0) & (kpis_hora['horas_RUN'] > 0),\n",
    "    (T_NOM / 3600) / (kpis_hora['horas_RUN'] / kpis_hora['N_botellas']),\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# 3. Quality = pct_tolerancia / 100\n",
    "kpis_hora['Quality'] = kpis_hora['pct_tolerancia'] / 100\n",
    "\n",
    "# 4. OEE = Availability × Performance × Quality\n",
    "kpis_hora['OEE'] = kpis_hora['Availability'] * kpis_hora['Performance'] * kpis_hora['Quality']\n",
    "\n",
    "# Convertir a porcentaje\n",
    "kpis_hora['OEE_pct'] = (kpis_hora['OEE'] * 100).round(2)\n",
    "\n",
    "print(f\"\\nPrimeras horas con OEE:\")\n",
    "print(kpis_hora[['N_botellas', 'Availability', 'Performance', 'Quality', 'OEE_pct']].head(10))\n",
    "\n",
    "# ============================================================================\n",
    "# PARTE B: OEE POR TURNO\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"--- OEE POR TURNO ---\")\n",
    "\n",
    "# 1. Availability = horas_RUN / horas_planificadas\n",
    "kpis_turno['Availability'] = kpis_turno['horas_RUN'] / HORAS_PLANIFICADAS_TURNO\n",
    "\n",
    "# 2. Performance = t_nom / t_medio_RUN\n",
    "kpis_turno['Performance'] = np.where(\n",
    "    (kpis_turno['N_botellas'] > 0) & (kpis_turno['horas_RUN'] > 0),\n",
    "    (T_NOM / 3600) / (kpis_turno['horas_RUN'] / kpis_turno['N_botellas']),\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# 3. Quality = pct_tolerancia / 100\n",
    "kpis_turno['Quality'] = kpis_turno['pct_tolerancia'] / 100\n",
    "\n",
    "# 4. OEE = Availability × Performance × Quality\n",
    "kpis_turno['OEE'] = kpis_turno['Availability'] * kpis_turno['Performance'] * kpis_turno['Quality']\n",
    "\n",
    "# Convertir a porcentaje\n",
    "kpis_turno['OEE_pct'] = (kpis_turno['OEE'] * 100).round(2)\n",
    "\n",
    "print(f\"\\nPrimeros turnos con OEE:\")\n",
    "print(kpis_turno[['N_botellas', 'Availability', 'Performance', 'Quality', 'OEE_pct']].head(10))\n",
    "\n",
    "print(f\"\\n✅ OEE calculado (Opción A)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "# FASE 3 — Análisis numérico (NumPy puro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "## Preparación y notación\n",
    "\n",
    "**Rejilla temporal:** Por-ciclo (cada botella alineada con telemetría más próxima)\n",
    "\n",
    "**Variables continuas:**\n",
    "- `T` = temp_prod (°C)\n",
    "- `q` = caudal (ml/s)\n",
    "- `P` = P_kW (kW)\n",
    "- `tc` = tiempo_ciclo_s (s)\n",
    "\n",
    "**Variable binaria:**\n",
    "- `RUN ∈ {0,1}` (1 si en marcha)\n",
    "\n",
    "**Error de llenado:**\n",
    "- `e = peso_lleno_g - m_obj(f)` donde `m_obj(250)=250g`, `m_obj(500)=500g`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 1.1: Calcular error de llenado\n",
    "print(\"=\"*60)\n",
    "print(\"FASE 3 - ANÁLISIS NUMÉRICO (NumPy puro)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nPASO 1.1: Cálculo del error de llenado\")\n",
    "\n",
    "# e = peso_lleno_g - m_obj(f)\n",
    "df_pz['error_llenado'] = df_pz['peso_lleno_g'] - df_pz['masa_objetivo']\n",
    "\n",
    "print(f\"\\nError de llenado calculado:\")\n",
    "print(f\"   Media: {df_pz['error_llenado'].mean():.3f} g\")\n",
    "print(f\"   Std: {df_pz['error_llenado'].std():.3f} g\")\n",
    "print(f\"   Min: {df_pz['error_llenado'].min():.3f} g\")\n",
    "print(f\"   Max: {df_pz['error_llenado'].max():.3f} g\")\n",
    "\n",
    "print(f\"\\n✅ Error de llenado calculado\")\n",
    "\n",
    "# PASO 1.2: Calcular tiempo de ciclo\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PASO 1.2: Cálculo del tiempo de ciclo\")\n",
    "\n",
    "# tc = diferencia temporal entre botellas consecutivas (en segundos)\n",
    "df_pz['tiempo_ciclo_s'] = df_pz['ts_ciclo'].diff().dt.total_seconds()\n",
    "\n",
    "# Estadísticas (ignorar primer valor NaN)\n",
    "tc_validos = df_pz['tiempo_ciclo_s'].dropna()\n",
    "\n",
    "print(f\"\\nTiempo de ciclo calculado:\")\n",
    "print(f\"   Media: {tc_validos.mean():.3f} s\")\n",
    "print(f\"   Mediana: {tc_validos.median():.3f} s\")\n",
    "print(f\"   Std: {tc_validos.std():.3f} s\")\n",
    "print(f\"   Min: {tc_validos.min():.3f} s\")\n",
    "print(f\"   Max: {tc_validos.max():.3f} s\")\n",
    "\n",
    "print(f\"\\n✅ Tiempo de ciclo calculado\")\n",
    "\n",
    "\n",
    "# PASO 1.3: Alinear telemetría con botellas\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PASO 1.3: Alineación telemetría con botellas (merge_asof)\")\n",
    "\n",
    "# Preparar df_tel: asegurar que el índice se llame 'ts' y reset_index\n",
    "df_tel.index.name = 'ts'  # Asegurar nombre del índice\n",
    "df_tel_temp = df_tel[['temp_prod', 'caudal', 'P_kW', 'estado']].reset_index()\n",
    "\n",
    "# Alinear cada botella con la muestra de telemetría más cercana ANTES del ciclo\n",
    "df_merge = pd.merge_asof(\n",
    "    df_pz.sort_values('ts_ciclo'),\n",
    "    df_tel_temp.sort_values('ts'),\n",
    "    left_on='ts_ciclo',\n",
    "    right_on='ts',\n",
    "    direction='backward',\n",
    "    tolerance=pd.Timedelta(seconds=5)  # Máximo 5 segundos de diferencia\n",
    ")\n",
    "\n",
    "# Convertir estado a binario: RUN=1, STOP=0\n",
    "df_merge['RUN'] = (df_merge['estado'] == 'RUN').astype(int)\n",
    "\n",
    "print(f\"\\nAlineación completada:\")\n",
    "print(f\"   Total de botellas: {len(df_merge):,}\")\n",
    "print(f\"   Botellas con telemetría válida: {df_merge['temp_prod'].notna().sum():,}\")\n",
    "\n",
    "print(f\"\\nPrimeros registros alineados:\")\n",
    "print(df_merge[['ts_ciclo', 'temp_prod', 'caudal', 'P_kW', 'tiempo_ciclo_s', 'error_llenado', 'RUN']].head(10))\n",
    "\n",
    "print(f\"\\n✅ Datos alineados\")\n",
    "\n",
    "\n",
    "# PASO 1.4: Extraer arrays NumPy y crear máscara de datos válidos\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PASO 1.4: Extracción a NumPy y máscara de validez\")\n",
    "\n",
    "# Extraer arrays NumPy (a partir de aquí solo NumPy)\n",
    "T = df_merge['temp_prod'].values\n",
    "q = df_merge['caudal'].values\n",
    "P = df_merge['P_kW'].values\n",
    "tc = df_merge['tiempo_ciclo_s'].values\n",
    "e = df_merge['error_llenado'].values\n",
    "RUN = df_merge['RUN'].values\n",
    "\n",
    "# Crear máscara de datos válidos (sin NaN en ninguna variable)\n",
    "mask_validos = ~(np.isnan(T) | np.isnan(q) | np.isnan(P) | np.isnan(tc) | np.isnan(e))\n",
    "\n",
    "# Filtrar arrays con la máscara\n",
    "T_clean = T[mask_validos]\n",
    "q_clean = q[mask_validos]\n",
    "P_clean = P[mask_validos]\n",
    "tc_clean = tc[mask_validos]\n",
    "e_clean = e[mask_validos]\n",
    "RUN_clean = RUN[mask_validos]\n",
    "\n",
    "n_total = len(T)\n",
    "n_validos = len(T_clean)\n",
    "\n",
    "print(f\"\\nMáscara de datos válidos:\")\n",
    "print(f\"   Total de registros: {n_total:,}\")\n",
    "print(f\"   Registros válidos (sin NaN): {n_validos:,} ({n_validos/n_total*100:.2f}%)\")\n",
    "print(f\"   Registros con NaN: {n_total - n_validos:,}\")\n",
    "\n",
    "print(f\"\\n✅ Arrays NumPy preparados: T, q, P, tc, e, RUN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## PASO 2: Correlaciones de Pearson\n",
    "\n",
    "### Fórmula de correlación de Pearson\n",
    "\n",
    "$$r_{xy} = \\frac{\\text{cov}(x,y)}{\\sigma_x \\sigma_y} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i-\\bar{x})^2} \\sqrt{\\sum(y_i-\\bar{y})^2}}$$\n",
    "\n",
    "**Implementación con estandarización:**\n",
    "\n",
    "1. Estandarizar: $z = \\frac{x - \\bar{x}}{\\sigma_x}$\n",
    "2. Matriz de correlación: $\\mathbf{R} = \\frac{\\mathbf{X}_{\\text{std}}^T \\mathbf{X}_{\\text{std}}}{n-1}$\n",
    "\n",
    "donde $\\mathbf{X}_{\\text{std}}$ es la matriz de datos estandarizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 2: Correlaciones de Pearson (NumPy puro)\n",
    "print(\"=\"*60)\n",
    "print(\"PASO 2: CORRELACIONES DE PEARSON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Función para estandarizar (z-score)\n",
    "def estandarizar(x):\n",
    "    \"\"\"Estandariza un array: (x - media) / std\"\"\"\n",
    "    return (x - np.mean(x)) / np.std(x)\n",
    "\n",
    "# Estandarizar todas las variables\n",
    "T_std = estandarizar(T_clean)\n",
    "q_std = estandarizar(q_clean)\n",
    "P_std = estandarizar(P_clean)\n",
    "tc_std = estandarizar(tc_clean)\n",
    "e_std = estandarizar(e_clean)\n",
    "\n",
    "# Construir matriz de datos estandarizados [T, q, P, tc, e]\n",
    "X_std = np.column_stack([T_std, q_std, P_std, tc_std, e_std])\n",
    "\n",
    "# Calcular matriz de correlación: R = (X'X) / (n-1)\n",
    "n = len(T_clean)\n",
    "corr_matrix = (X_std.T @ X_std) / (n - 1)\n",
    "\n",
    "# Nombres de variables\n",
    "var_names = ['T', 'q', 'P', 'tc', 'e']\n",
    "\n",
    "print(f\"\\nMatriz de correlación de Pearson ({n:,} muestras):\")\n",
    "print(\"\\n\" + \" \"*8 + \"\".join(f\"{v:>8}\" for v in var_names))\n",
    "print(\"-\" * 48)\n",
    "for i, nombre in enumerate(var_names):\n",
    "    fila = \"\".join(f\"{corr_matrix[i,j]:>8.3f}\" for j in range(len(var_names)))\n",
    "    print(f\"{nombre:>8}{fila}\")\n",
    "\n",
    "# Identificar correlaciones más fuertes con el error (e)\n",
    "print(f\"\\nCorrelaciones con el error de llenado (e):\")\n",
    "idx_e = 4  # Índice de 'e' en var_names\n",
    "for i, var in enumerate(var_names[:-1]):  # Excluir 'e' mismo\n",
    "    print(f\"   {var} vs e: {corr_matrix[i, idx_e]:>7.3f}\")\n",
    "\n",
    "print(f\"\\n✅ Correlaciones calculadas con NumPy puro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## PASO 3: Regresión Lineal OLS (NumPy puro)\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "Ajustar un **modelo de regresión lineal múltiple** para explicar el **error de llenado** `e` en función de las variables físicas del proceso.\n",
    "\n",
    "---\n",
    "\n",
    "### Modelo matemático\n",
    "\n",
    "$$e = \\beta_0 + \\beta_1 T + \\beta_2 q + \\beta_3 P + \\beta_4 \\text{RUN} + \\varepsilon$$\n",
    "\n",
    "**Donde:**\n",
    "- **Variable dependiente (y):** `e` = error de llenado (gramos)\n",
    "- **Variables independientes (X):**\n",
    "  - `T` = Temperatura del producto (°C)\n",
    "  - `q` = Caudal (ml/s)\n",
    "  - `P` = Potencia instantánea (kW)\n",
    "  - `RUN` = Estado de la máquina (1=marcha, 0=parada)\n",
    "- **ε:** Error aleatorio\n",
    "\n",
    "---\n",
    "\n",
    "### Fórmula de estimación OLS\n",
    "\n",
    "**Matriz de diseño:**\n",
    "\n",
    "$$\\mathbf{X} = \\begin{bmatrix} \n",
    "1 & T_1 & q_1 & P_1 & \\text{RUN}_1 \\\\\n",
    "1 & T_2 & q_2 & P_2 & \\text{RUN}_2 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & T_n & q_n & P_n & \\text{RUN}_n\n",
    "\\end{bmatrix}, \\quad\n",
    "\\mathbf{y} = \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}$$\n",
    "\n",
    "**Estimador de mínimos cuadrados:**\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$$\n",
    "\n",
    "---\n",
    "\n",
    "### Métricas de calidad del ajuste\n",
    "\n",
    "**1. Coeficiente de determinación R²:**\n",
    "\n",
    "$$R^2 = 1 - \\frac{SS_{\\text{res}}}{SS_{\\text{tot}}}$$\n",
    "\n",
    "Donde:\n",
    "- $SS_{\\text{res}} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ = Suma de cuadrados de residuos\n",
    "- $SS_{\\text{tot}} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2$ = Suma de cuadrados total\n",
    "\n",
    "**Interpretación:** % de varianza explicada por el modelo.\n",
    "\n",
    "---\n",
    "\n",
    "**2. R² ajustado:**\n",
    "\n",
    "$$R^2_{\\text{adj}} = 1 - (1 - R^2) \\cdot \\frac{n-1}{n-p-1}$$\n",
    "\n",
    "Donde:\n",
    "- $n$ = número de observaciones\n",
    "- $p$ = número de predictores (sin contar intercepto)\n",
    "\n",
    "**Interpretación:** Penaliza la adición de predictores que no mejoran significativamente el modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### Diagnósticos mínimos\n",
    "\n",
    "**Media de residuos:**\n",
    "\n",
    "$$\\bar{\\varepsilon} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i) \\approx 0$$\n",
    "\n",
    "**Interpretación física esperada de los coeficientes:**\n",
    "\n",
    "| Coeficiente | Signo esperado | Razón física |\n",
    "|-------------|----------------|--------------|\n",
    "| β₁ (T) | **Negativo (−)** | ↑ temperatura → ↓ viscosidad → fluye más rápido → subllenado |\n",
    "| β₂ (q) | **Positivo (+)** | ↑ caudal → mayor flujo → sobrellenado |\n",
    "| β₃ (P) | **≈ 0 o pequeño** | Efecto indirecto; relacionado con velocidad de cinta |\n",
    "| β₄ (RUN) | **Negativo (−)** | Transiciones STOP→RUN pueden capturar inestabilidades |\n",
    "\n",
    "---\n",
    "\n",
    "### Implementación\n",
    "\n",
    "El código aplicará:\n",
    "1. Construcción de matrices **X** (con columna de unos) e **y**\n",
    "2. Resolución del sistema **X'Xβ = X'y** usando `np.linalg.solve()`\n",
    "3. Cálculo de predicciones **ŷ = Xβ**\n",
    "4. Cálculo de residuos **ε = y - ŷ**\n",
    "5. Métricas **R²** y **R²_adj**\n",
    "6. Interpretación de signos y magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PASO 3: Regresión Lineal OLS (NumPy puro) - CON DIAGNÓSTICO\n",
    "print(\"=\"*60)\n",
    "print(\"PASO 3: REGRESIÓN LINEAL OLS (NumPy puro)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================================\n",
    "# 3.1: CONSTRUCCIÓN DE MATRICES X e y\n",
    "# ============================================================================\n",
    "print(\"\\n--- 3.1: Construcción de matrices ---\")\n",
    "\n",
    "# Matriz de diseño X: [1, T, q, P, RUN]\n",
    "n = len(T_clean)\n",
    "X_ols = np.column_stack([\n",
    "    np.ones(n),      # β₀ (intercepto)\n",
    "    T_clean,         # β₁ (temperatura)\n",
    "    q_clean,         # β₂ (caudal)\n",
    "    P_clean,         # β₃ (potencia)\n",
    "    RUN_clean        # β₄ (estado RUN/STOP)\n",
    "])\n",
    "\n",
    "y_ols = e_clean\n",
    "\n",
    "print(f\"Matriz X: {X_ols.shape} (n={n}, p={X_ols.shape[1]-1} predictores + intercepto)\")\n",
    "print(f\"Vector y: {y_ols.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# DIAGNÓSTICO: Verificar varianza y colinealidad\n",
    "# ============================================================================\n",
    "print(\"\\n--- DIAGNÓSTICO ---\")\n",
    "\n",
    "var_names = ['Intercepto', 'T', 'q', 'P', 'RUN']\n",
    "print(f\"\\nEstadísticas de cada columna de X:\")\n",
    "for i, name in enumerate(var_names):\n",
    "    col = X_ols[:, i]\n",
    "    print(f\"   {name:12s}: min={np.min(col):>8.3f}, max={np.max(col):>8.3f}, \"\n",
    "          f\"std={np.std(col):>8.3f}, unique={len(np.unique(col)):>5}\")\n",
    "\n",
    "# Verificar si hay columnas constantes (std ≈ 0)\n",
    "stds = np.std(X_ols, axis=0)\n",
    "columnas_constantes = np.where(stds < 1e-10)[0]\n",
    "\n",
    "if len(columnas_constantes) > 1:  # > 1 porque intercepto siempre es constante\n",
    "    print(f\"\\n⚠️  PROBLEMA: Columnas con varianza ≈0 detectadas:\")\n",
    "    for idx in columnas_constantes:\n",
    "        print(f\"   - {var_names[idx]} (std={stds[idx]:.10f})\")\n",
    "    print(\"\\nSOLUCIÓN: Eliminar variable(s) constante(s)\")\n",
    "\n",
    "# Verificar condición de la matriz X'X\n",
    "XtX = X_ols.T @ X_ols\n",
    "cond_number = np.linalg.cond(XtX)\n",
    "print(f\"\\nNúmero de condición de X'X: {cond_number:.2e}\")\n",
    "\n",
    "if cond_number > 1e10:\n",
    "    print(\"⚠️  Matriz mal condicionada (multicolinealidad o columnas constantes)\")\n",
    "    print(\"   Solución: Usar pseudo-inversa (np.linalg.lstsq)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3.2: ESTIMACIÓN DE COEFICIENTES β (con manejo robusto)\n",
    "# ============================================================================\n",
    "print(\"\\n--- 3.2: Estimación de coeficientes ---\")\n",
    "\n",
    "try:\n",
    "    # Intentar método estándar\n",
    "    Xty = X_ols.T @ y_ols\n",
    "    beta = np.linalg.solve(XtX, Xty)\n",
    "    metodo = \"solve() directo\"\n",
    "    \n",
    "except np.linalg.LinAlgError:\n",
    "    # Si falla, usar mínimos cuadrados con pseudo-inversa\n",
    "    print(\"⚠️  solve() falló (matriz singular)\")\n",
    "    print(\"   Usando np.linalg.lstsq() (pseudo-inversa)\")\n",
    "    \n",
    "    beta, residuals, rank, s = np.linalg.lstsq(X_ols, y_ols, rcond=None)\n",
    "    metodo = f\"lstsq() - rank={rank}/{X_ols.shape[1]}\"\n",
    "    \n",
    "    if rank < X_ols.shape[1]:\n",
    "        print(f\"   ⚠️  Rango deficiente: {rank}/{X_ols.shape[1]}\")\n",
    "        print(f\"   Algunas variables pueden ser redundantes\")\n",
    "\n",
    "# Nombres de los coeficientes\n",
    "coef_names = ['β₀ (intercepto)', 'β₁ (T)', 'β₂ (q)', 'β₃ (P)', 'β₄ (RUN)']\n",
    "\n",
    "print(f\"\\nMétodo usado: {metodo}\")\n",
    "print(f\"\\nCoeficientes estimados:\")\n",
    "for name, coef in zip(coef_names, beta):\n",
    "    print(f\"   {name:20s}: {coef:>10.6f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3.3: PREDICCIONES Y RESIDUOS\n",
    "# ============================================================================\n",
    "print(\"\\n--- 3.3: Predicciones y residuos ---\")\n",
    "\n",
    "y_pred = X_ols @ beta\n",
    "residuos = y_ols - y_pred\n",
    "\n",
    "print(f\"\\nEstadísticas de residuos:\")\n",
    "print(f\"   Media: {np.mean(residuos):.6f} (debe ser ≈0)\")\n",
    "print(f\"   Std: {np.std(residuos):.3f}\")\n",
    "print(f\"   Min: {np.min(residuos):.3f}\")\n",
    "print(f\"   Max: {np.max(residuos):.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3.4: MÉTRICAS DE CALIDAD DEL AJUSTE\n",
    "# ============================================================================\n",
    "print(\"\\n--- 3.4: Calidad del ajuste ---\")\n",
    "\n",
    "SS_res = np.sum(residuos**2)\n",
    "SS_tot = np.sum((y_ols - np.mean(y_ols))**2)\n",
    "R2 = 1 - (SS_res / SS_tot)\n",
    "\n",
    "p = X_ols.shape[1] - 1\n",
    "R2_adj = 1 - (1 - R2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "print(f\"\\nR² = {R2:.4f} ({R2*100:.2f}% de varianza explicada)\")\n",
    "print(f\"R²_adj = {R2_adj:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3.5: INTERPRETACIÓN DE COEFICIENTES\n",
    "# ============================================================================\n",
    "print(\"\\n--- 3.5: Interpretación física ---\")\n",
    "\n",
    "print(f\"\\nSignos esperados según teoría:\")\n",
    "print(f\"   β₁ (T) < 0: ↑ temperatura → ↓ viscosidad → ↓ peso (subllenado)\")\n",
    "print(f\"   β₂ (q) > 0: ↑ caudal → ↑ peso (sobrellenado)\")\n",
    "print(f\"   β₃ (P) ≈ 0: Potencia tiene efecto indirecto\")\n",
    "print(f\"   β₄ (RUN) < 0: Estado STOP puede capturar cambios de régimen\")\n",
    "\n",
    "print(f\"\\nSignos obtenidos:\")\n",
    "for i, (name, coef) in enumerate(zip(coef_names[1:], beta[1:]), 1):\n",
    "    signo = \"+\" if coef > 0 else \"-\"\n",
    "    coincide = \"✓\" if (\n",
    "        (i == 1 and coef < 0) or  # β₁(T) negativo\n",
    "        (i == 2 and coef > 0) or  # β₂(q) positivo\n",
    "        (i == 4 and coef < 0)     # β₄(RUN) negativo\n",
    "    ) else \"✗\"\n",
    "    print(f\"   {name:15s}: {signo} ({coef:>10.6f}) {coincide}\")\n",
    "\n",
    "print(f\"\\n✅ Regresión OLS completada\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
